---
title: "Creator Gender Fairness in Mystery/Thriller/Crime Book Recommendations"
author: "Group 13"
date: "12/21/2021"
output:
  pdf_document: default
  html_document: default
---


```{r, eval=FALSE}
install.packages("recommenderlab")
install.packages("rstudioapi")
install.packages("dplyr")
install.packages("ggplot2")
```
#Set working directory

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

setwd(dirname(rstudioapi::getSourceEditorContext()$path))
```


```{r}
suppressPackageStartupMessages({
  library(jsonlite)
  library(dplyr)
  library(WikidataR)
  library(recommenderlab)
  library(boot)
  library(ggplot2)
})
```

## Citations


UCSD Bookgraph dataset:
    * Mengting Wan, Julian McAuley, "Item Recommendation on Monotonic Behavior Chains", in RecSys'18.
    * Mengting Wan, Rishabh Misra, Ndapa Nakashole, Julian McAuley, "Fine-Grained Spoiler Detection from Large-Scale Review Corpora", in ACL'19.

Recommenderlab
* Michael Hahsler (2021). recommenderlab: Lab for Developing and Testing Recommender Algorithms. R package version 0.2-7.




## Constants

```{r}
GOOD_RATING = 4
MIN_REVIEWS_OF_BOOK = 50
MIN_REVIEWS_BY_USER = 50
BOOK_GENRE = "comics_graphic"
NUM_BOOT_REPLICATES = 10
N_PERMUTATIONS = 10000
```


## Load Data 

```{r readAuthors}
books_genre_df <- read.csv( 
  sprintf("data/goodreads_books_%s.csv", BOOK_GENRE) 
)
authors_genre_df <- read.csv( 
  sprintf("data/goodreads_books_authors_%s.csv", BOOK_GENRE) 
)

books_authors_genre <- inner_join(
  authors_genre_df, 
  books_genre_df, 
  by="author_id"
)

ggplot(books_authors_genre, aes(x=gender)) +
  geom_bar( 
    stat="count", 
    fill="forestgreen", 
    alpha=0.5) +
  ggtitle( sprintf("Amount of books by %s Author Gender", BOOK_GENRE) ) +
  geom_text(
    aes(label = ..count..), 
    stat = "count",
    vjust = 1.5, 
    colour = "black"
  )

```

Though we have non-binary gender values, due to the nature of our analysis, 
we will have to restrict to a gender binary!

#Filter for binary gender
```{r}
books_authors_genre %>%
  filter(gender == "FEMALE" | gender == "MALE") -> books_authors_genre

ggplot(books_authors_genre, aes(x=gender)) +
  geom_bar( 
    stat="count", 
    fill="forestgreen", 
    alpha=0.5) +
  ggtitle( sprintf("Amount of %s books by Authors Genders", BOOK_GENRE) ) +
  geom_text(
    aes(label = ..count..), 
    stat = "count",
    vjust = 1.5, 
    colour = "black"
  )

print(c("Amount of books:", nrow(books_authors_genre)))
```

Now we have a gender binary! Women clearly outnumber men in terms of books. In total there are 221,809 books.

# Plot the distribution of ratings

```{r}
reviews_genre_df <- read.csv( 
  sprintf("data/goodreads_reviews_%s.csv", BOOK_GENRE) 
)

# Rating 0 means unrated;  Remove
reviews_genre_df %>%
  filter(rating > 0) -> reviews_genre_df

ggplot(reviews_genre_df, aes(x=rating)) + 
  geom_histogram(fill = "forestgreen")
```

We observe that the ratings are skewed positive!

# Join the books- and reviews dataset
```{r}
inner_join( 
  reviews_genre_df, 
  books_authors_genre, 
  by="book_id" 
) -> genre_books_reviews 

ggplot(genre_books_reviews, aes(x=gender)) +
  geom_bar( 
    stat="count", 
    fill="forestgreen", 
    alpha=0.5) +
  ggtitle( sprintf("amount of reviews by %s authors gender", BOOK_GENRE) ) + 
  geom_text(
    aes(label = ..count..), 
    stat = "count",
    vjust = 1.5, 
    colour = "black"
  )

print(c("Amount of reviews:", nrow(genre_books_reviews)))
```
Similarly, as for the distribution of books, more reviews exist among female authors. Here the difference between the genders is even greater. In total we have 2,948,561 reviews

# Convert rating: number to class
```{r}
genre_books_reviews <- genre_books_reviews %>%
  mutate(like_or_dislike =  
    case_when( 
       rating >= GOOD_RATING ~ "LIKE", 
       TRUE ~ "DISLIKE"
    )
  )
```


#Filter users with less then 50 reviews
Let's ensure that there are only those users with at least `MIN_REVIEWS_BY_USER` 
reviews

```{r}
genre_books_reviews %>%
  add_count(user_id, name="n_reviews_by_user") %>%
  filter(n_reviews_by_user >= MIN_REVIEWS_BY_USER) %>%
  arrange(desc(n_reviews_by_user)) -> genre_books_reviews

print(c("Amount of reviews:", nrow(genre_books_reviews)))

head(
  genre_books_reviews %>% 
    select(user_id, book_id, rating, n_reviews_by_user) %>% 
    arrange(n_reviews_by_user)
)
ggplot(genre_books_reviews, aes(x=gender)) +
  geom_bar( 
    stat="count", 
    fill="forestgreen", 
    alpha=0.5) +
  ggtitle( sprintf("amount of reviews by %s authors gender, after filtering n_reviews_by_user > 50", BOOK_GENRE) ) + 
  geom_text(
    aes(label = ..count..), 
    stat = "count",
    vjust = 1.5, 
    colour = "black"
  )
```
After filtering the imbalance is still here. There are 1,482,662 reviews left

#Filter books with less then 50 ratings

```{r}
genre_books_reviews %>%
  add_count(book_id, name="n_reviews_of_book") %>%
  filter(n_reviews_of_book >= MIN_REVIEWS_OF_BOOK) -> genre_books_reviews

print(c("Amount of reviews:", nrow(genre_books_reviews)))

head(
  genre_books_reviews %>% 
    select(user_id, book_id, rating, n_reviews_of_book) %>% 
    arrange(n_reviews_of_book)
)

ggplot(genre_books_reviews, aes(x=gender)) +
  geom_bar( 
    stat="count", 
    fill="forestgreen", 
    alpha=0.5) + 
  ggtitle( sprintf("Amount of %s reviews by genders, after filtering n_reviews_of_book > 50", BOOK_GENRE) ) + 
  geom_text(
    aes(label = ..count..), 
    stat = "count",
    vjust = 1.5, 
    colour = "black"
  )
```
After filtering books with less then 50 reviews there are 741,725 reviews left.

Finally, we should treat The fields user_id and book_id should be treated as 
factors
```{r}
genre_books_reviews$user_id <- as.factor(genre_books_reviews$user_id)
genre_books_reviews$book_id <- as.factor(genre_books_reviews$book_id)
```

## Recall, Precision and F1

In order to compare algorithms' performance for each genre, 
we use F1 as performance metric.
```{r}
getRecall <- function(the_df) {
  true_positives <- sum(
    the_df$like_or_dislike == "LIKE" & the_df$prediction == "LIKE"
  )
  false_negatives <- sum(
    the_df$like_or_dislike == "LIKE" & the_df$prediction != "LIKE"
  )
  return ( (true_positives)/(true_positives+false_negatives) )
}

getPrecision <- function(the_df) {
  true_positives <- sum(
    the_df$like_or_dislike == "LIKE" & the_df$prediction == "LIKE"
    )
  false_positives <- sum(
    the_df$like_or_dislike != "LIKE" & the_df$prediction == "LIKE"
  )
  return( true_positives/(true_positives + false_positives) )
}

getF1 <- function(the_df) {
  precision = getPrecision(the_df)
  recall = getRecall(the_df)
  return( 2 * (precision * recall) / (precision + recall) )
}
```

#reduce samplesize for shorter runtime

```{r}
genre_books_reviews = genre_books_reviews[sample(nrow(genre_books_reviews),50000),]
```


## Bootstrap

```{r warnings=FALSE}
ReturnRecgap <- function(reviews_df, sampleindices, the_algo)
{
  tryCatch(
    expr = {
      # reviews_df will contain the dataframe.  
      sampled_reviews_df <- reviews_df[sampleindices, ]
      
      # make a realRatingMatrix out of it and create an evaluation scheme
      sampled_reviews_df$user <- as.factor(sampled_reviews_df$user_id)
      sampled_reviews_df$item <- as.factor(sampled_reviews_df$book_id)
      sampled_reviews_df$rating <- as.numeric(sampled_reviews_df$rating)
      sampled_reviews_df %>% select(user, item, rating) -> sampled_reviews_df
      sampled_reviews_Matrix <- as(sampled_reviews_df, "realRatingMatrix")
      
      evaluationScheme(
         data = sampled_reviews_Matrix,
         method = "split",
         train = 0.9,
         given = -1,
         goodRating = GOOD_RATING
      ) -> evaluation_scheme_cf 
      
      train_data <- getData(evaluation_scheme_cf, "train")
      known_data <- getData(evaluation_scheme_cf, "known")
      
      # Sometimes, known_data has rows that are all NA!
      # This is especially true for UBCF
      # We remove these
      known_data <- as(known_data, "matrix")
      known_data <- known_data[rowSums(is.na(known_data)) != ncol(known_data),] 
      known_data <- as(known_data, "realRatingMatrix")
      
      the_recommender <- Recommender( train_data, the_algo )
      
      the_predictions <- predict( 
        the_recommender, 
        known_data , 
        type="ratings", 
        n = 5 
      )
      the_predictions_df <- as(the_predictions, "data.frame")
      
      the_predictions_df <- the_predictions_df %>% 
        rename(
          cf_rating = rating,
          user_id = user,
          book_id = item
        ) %>%
        mutate(prediction_cf = 
                 case_when( 
                   cf_rating >= GOOD_RATING ~ "LIKE", 
                   TRUE ~ "DISLIKE"
                 ))
      known_data_df <- as(known_data, "data.frame")
      
      cf_df <- inner_join(
        genre_books_reviews, 
        the_predictions_df, 
        by=c("user_id", "book_id")
      )
      
      cf_df_female <- cf_df %>% filter(gender == "FEMALE")
      cf_df_male <- cf_df %>% filter(gender == "MALE")
      
      overall_evaluationsscore_cf     <- getF1(cf_df)                        
      mean_evaluationscore_female_cf  <- getF1(cf_df_female)
      mean_evaluationscore_male_cf    <- getF1(cf_df_male)
      recgap_genre_cf <- mean_evaluationscore_female_cf - mean_evaluationscore_male_cf
      
      message("Successfully Run Bootstap iteration")
      
      ret <- c( overall_evaluationsscore_cf, recgap_genre_cf ) 
      names(ret) <- c( "performance", "recgap" )
      return(ret)
    },
    error = function(e) {
      overall_evaluationsscore_cf <- NA
      recgap_genre_cf <- NA
      message("Got an error.  Returning NA, NA!")
      
      ret <- c( overall_evaluationsscore_cf, recgap_genre_cf ) 
      names(ret) <- c( "performance", "recgap" )
      return(ret)
    }, finally = {
      message("In finally block")
    }
  )
}

genre_books_reviews %>% 
  select(user_id, book_id, rating, gender, like_or_dislike) -> sampled_reviews


get_cf_boot_results <- function( cf_algo ) {
  
   boot_results_cf <- boot(
    data = sampled_reviews, 
    statistic = ReturnRecgap, 
    R = NUM_BOOT_REPLICATES, 
    the_algo = cf_algo
  )
  
  as.data.frame(boot_results_cf$t) %>% 
    rename(performance = V1, recgap = V2) %>% 
    filter( !is.na(performance) & !is.na(recgap) ) -> results_cf
  
  results_cf$algo <- cf_algo
  
  filename <- sprintf(
    "output/%s_cf_boot_results_%s_%d.csv", 
    BOOK_GENRE, 
    cf_algo, 
    NUM_BOOT_REPLICATES
  )
  message("Writing boot results file: ", filename)
  write.csv(results_cf, filename)
  
  return ( results_cf )
}

```


## Item-Based Collaborative Filtering (IBCF)

Here, we used Item-Based CF in order to make recommendations.  IBCF assumes 
that users will prefer items similar to other items that they like.  The
recommenderlab package figures out that two items are similar by calculating 
the statistical distance between them.  Here, we are using the cosine distance
between them to find out this distance.

```{r warning=FALSE, message=FALSE}
ibcf_boot_results <- get_cf_boot_results("IBCF")

head(ibcf_boot_results)
```

## User-Based Collaborative Filtering (UBCF)

UBCF tries to mimic word-of-mouth recommendations.  If a user U1 likes Books B1
and B2, chances are that user U2 will also like these books, if U1 and U2 are
similar to each other.  To find out whether two users are similar, the
recommenderlab package users k-nearest-neighbours to find similar users, using
cosine or pearson similarity.

```{r warning=FALSE, message=FALSE}
ubcf_boot_results <- get_cf_boot_results("UBCF")

head(ubcf_boot_results)
```


## POP

This is just recommends everyone the same N books that are "most popular".  It
can be thought of as being similar to the Billboard Top 100 etc.

```{r warning=FALSE, message=FALSE}
pop_boot_results <- get_cf_boot_results("POP")

head(pop_boot_results)
```


## Compare the algorithms!

We can now compare the algorithms with each other, visually.  We have up to
`NUM_BOOT_REPLICATES` results for each algo.  We can compare the means, and 
get confidence intervals.

Credit:  https://www.r-graph-gallery.com/4-barplot-with-error-bar.html

```{r}
boot_results <- rbind(
  ibcf_boot_results, 
  ubcf_boot_results,
  pop_boot_results
)

sum_recgaps <- boot_results %>%
  group_by(algo) %>%
  summarise( 
    n=n(),
    mean_recgap=mean(recgap),
    sd_recgap=sd(recgap),
    mean_perf=mean(performance),
    sd_perf=sd(performance)
  ) %>%
  mutate( se=sd_recgap/sqrt(n))  %>%
  mutate( ic=se * qt((1-0.05)/2 + .5, n-1))
 

# Confidence Interval
ggplot(sum_recgaps) +
  geom_bar( 
    aes(x=algo, y=mean_recgap), 
    stat="identity", 
    fill="forestgreen", 
    alpha=0.5) +
  geom_errorbar( 
    aes(x=algo, ymin=mean_recgap-ic, ymax=mean_recgap+ic), 
    width=0.4, 
    colour="orange", 
    alpha=0.9, 
    size=1.5) +
  ggtitle("Comparison of mean bootstrapped recgap, whiskers indicate confidence interval")
```


## Recgap vs Performance

```{r}
ggplot(sum_recgaps, aes(x=mean_recgap, y=mean_perf, color=algo)) + 
    geom_point(size=6)
```


```{r}
cor(sum_recgaps$mean_perf, sum_recgaps$mean_recgap)
```

But is this correlation a pure coincidence?


```{r}
#TODO: Does this even make sense?
corPerm <- numeric(length = N_PERMUTATIONS)
for(i in 1:N_PERMUTATIONS)
{
 shufdata <- sum_recgaps[sample(nrow(sum_recgaps)),]
 corPerm[i] <- cor(shufdata$mean_perf, sum_recgaps$mean_recgap)
}
corObserved <- cor(sum_recgaps$mean_perf, sum_recgaps$mean_recgap)


p_value_Cor <- (sum(corPerm>=corObserved)+1)/length(corPerm)
print(c("p value:", p_value_Cor))

hist(corPerm, xlim=range(c(corPerm,corObserved)))
abline(v=corObserved, col="red")
```

From the p-value and the histogram, we can see that the relationship between 
the recgap and the performance is **not** significant and could have totally 
occurred by chance!
