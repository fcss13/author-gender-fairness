---
title: "Creator Gender Fairness in Mystery/Thriller/Crime Book Recommendations"
author: "Group 13"
date: "12/21/2021"
output:
  pdf_document: default
  html_document: default
---


```{r, eval=FALSE}
install.packages("recommenderlab")
install.packages("rstudioapi")
install.packages("dplyr")
install.packages("ggplot2")
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

setwd(dirname(rstudioapi::getSourceEditorContext()$path))
```


```{r}
suppressPackageStartupMessages({
  library(jsonlite)
  library(dplyr)
  library(WikidataR)
  library(recommenderlab)
  library(boot)
  library(ggplot2)
})
```

## Citations


UCSD Bookgraph dataset:
    * Mengting Wan, Julian McAuley, "Item Recommendation on Monotonic Behavior Chains", in RecSys'18.
    * Mengting Wan, Rishabh Misra, Ndapa Nakashole, Julian McAuley, "Fine-Grained Spoiler Detection from Large-Scale Review Corpora", in ACL'19.

Recommenderlab
* Michael Hahsler (2021). recommenderlab: Lab for Developing and Testing Recommender Algorithms. R package version 0.2-7.




## Constants

```{r}
GOOD_RATING = 4
MIN_REVIEWS_OF_BOOK = 50
MIN_REVIEWS_BY_USER = 50
BOOK_GENRE = "mystery_thriller_crime"
NUM_BOOT_REPLICATES = 10
N_PERMUTATIONS = 10000

N_BOOT_CPUS = 1
if (.Platform$OS.type == "unix") {
  N_BOOT_CPUS <- strtoi( system("nproc", intern=TRUE) )
} else {  # Windows
  N_BOOT_CPUS <- strtoi( Sys.getenv("NUMBER_OF_PROCESSORS") )
}
options( boot.ncpus = N_BOOT_CPUS )
options( boot.parallel = "multicore" )
```


## Load Data 

```{r readAuthors}
books_genre_df <- read.csv( 
  sprintf("data/goodreads_books_%s.csv", BOOK_GENRE) 
)
authors_genre_df <- read.csv( 
  sprintf("data/goodreads_books_authors_%s.csv", BOOK_GENRE) 
)

books_authors_genre <- inner_join(
  authors_genre_df, 
  books_genre_df, 
  by="author_id"
)

ggplot(books_authors_genre, aes(x=gender)) +
  geom_bar( 
    stat="count", 
    fill="forestgreen", 
    alpha=0.5) +
  ggtitle( sprintf("%s Author Genders", BOOK_GENRE) ) +
  geom_text(
    aes(label = ..count..), 
    stat = "count",
    vjust = 1.5, 
    colour = "black"
  )

```

Though we have non-binary gender values, due to the nature of our analysis, 
we will have to restrict to a gender binary!


```{r}
books_authors_genre %>%
  filter(gender == "FEMALE" | gender == "MALE") -> books_authors_genre

ggplot(books_authors_genre, aes(x=gender)) +
  geom_bar( 
    stat="count", 
    fill="forestgreen", 
    alpha=0.5) +
  ggtitle( sprintf("%s Author (Binary) Genders", BOOK_GENRE) ) +
  geom_text(
    aes(label = ..count..), 
    stat = "count",
    vjust = 1.5, 
    colour = "black"
  )
```

Now we have a gender binary! Men clearly outnumber women in terms of authors.

```{r}
reviews_genre_df <- read.csv( 
  sprintf("data/goodreads_reviews_%s.csv", BOOK_GENRE) 
)

# Rating 0 means unrated;  Remove
reviews_genre_df %>%
  filter(rating > 0) -> reviews_genre_df

ggplot(reviews_genre_df, aes(x=rating)) + 
  geom_histogram(fill = "forestgreen")
```

We observe that the ratings are skewed positive!


```{r}
inner_join( 
  reviews_genre_df, 
  books_authors_genre, 
  by="book_id" 
) -> genre_books_reviews 

ggplot(genre_books_reviews, aes(x=gender)) +
  geom_bar( 
    stat="count", 
    fill="forestgreen", 
    alpha=0.5) +
  ggtitle( sprintf("%s Author (Binary) Genders", BOOK_GENRE) ) +
  geom_text(
    aes(label = ..count..), 
    stat = "count",
    vjust = 1.5, 
    colour = "black"
  )

```
Even though the authors themselves were unevenly distributed, authors with
reviews are evenly distributed for this genre!

```{r}
genre_books_reviews <- genre_books_reviews %>%
  mutate(like_or_dislike =  
    case_when( 
       rating >= GOOD_RATING ~ "LIKE", 
       TRUE ~ "DISLIKE"
    )
  )
```


Let's ensure that there are only those users with at least `MIN_REVIEWS_BY_USER` 
reviews

```{r}
genre_books_reviews %>%
  add_count(user_id, name="n_reviews_by_user") %>%
  filter(n_reviews_by_user >= MIN_REVIEWS_BY_USER) %>%
  arrange(desc(n_reviews_by_user)) -> genre_books_reviews

head(
  genre_books_reviews %>% 
    select(user_id, book_id, rating, n_reviews_by_user) %>% 
    arrange(n_reviews_by_user)
)
```

```{r}
genre_books_reviews %>%
  add_count(book_id, name="n_reviews_of_book") %>%
  filter(n_reviews_of_book >= MIN_REVIEWS_OF_BOOK) -> genre_books_reviews

head(
  genre_books_reviews %>% 
    select(user_id, book_id, rating, n_reviews_of_book) %>% 
    arrange(n_reviews_of_book)
)
```


Finally, we should treat The fields user_id and book_id should be treated as 
factors
```{r}
genre_books_reviews$user_id <- as.factor(genre_books_reviews$user_id)
genre_books_reviews$book_id <- as.factor(genre_books_reviews$book_id)
```

## Recall

In order to compare algorithms' performance for each genre, we need to find the 
recall.

```{r}
getRecall <- function(the_df) {
  true_positives <- sum(
    the_df$like_or_dislike=="LIKE" & the_df$prediction == "LIKE"
  )
  false_negatives <- sum(
    the_df$like_or_dislike=="LIKE" & the_df$prediction != "LIKE"
  )
  return ( (true_positives)/(true_positives+false_negatives) )
}
```

## Bootstrap

```{r warnings=FALSE, eval=FALSE}
ReturnRecgap <- function(reviews_df, sampleindices, the_algo)
{
  tryCatch(
    expr = {
      # reviews_df will contain the dataframe.  
      sampled_reviews_df <- reviews_df[sampleindices, ]
      
      # make a realRatingMatrix out of it and create an evaluation scheme
      sampled_reviews_df$user <- as.factor(sampled_reviews_df$user_id)
      sampled_reviews_df$item <- as.factor(sampled_reviews_df$book_id)
      sampled_reviews_df$rating <- as.numeric(sampled_reviews_df$rating)
      sampled_reviews_df %>% select(user, item, rating) -> sampled_reviews_df
      sampled_reviews_Matrix <- as(sampled_reviews_df, "realRatingMatrix")
      
      evaluationScheme(
         data = sampled_reviews_Matrix,
         method = "split",
         train = 0.9,
         given = -1,
         goodRating = GOOD_RATING
      ) -> evaluation_scheme_cf 
      
      train_data <- getData(evaluation_scheme_cf, "train")
      known_data <- getData(evaluation_scheme_cf, "known")
      
      # Sometimes, known_data has rows that are all NA!
      # This is especially true for UBCF
      # We remove these
      known_data <- as(known_data, "matrix")
      known_data <- known_data[rowSums(is.na(known_data)) != ncol(known_data),] 
      known_data <- as(known_data, "realRatingMatrix")
      
      the_recommender <- Recommender( train_data, the_algo )
      
      the_predictions <- predict( 
        the_recommender, 
        known_data , 
        type="ratings", 
        n = 5 
      )
      the_predictions_df <- as(the_predictions, "data.frame")
      
      the_predictions_df <- the_predictions_df %>% 
        rename(
          cf_rating = rating,
          user_id = user,
          book_id = item
        ) %>%
        mutate(prediction_cf = 
                 case_when( 
                   cf_rating >= GOOD_RATING ~ "LIKE", 
                   TRUE ~ "DISLIKE"
                 ))
      known_data_df <- as(known_data, "data.frame")
      
      cf_df <- inner_join(
        genre_books_reviews, 
        the_predictions_df, 
        by=c("user_id", "book_id")
      )
      
      cf_df_female <- cf_df %>% filter(gender == "FEMALE")
      cf_df_male <- cf_df %>% filter(gender == "MALE")
      
      overall_evaluationsscore_cf     <- getRecall(cf_df)
      mean_evaluationscore_female_cf  <- getRecall(cf_df_female)
      mean_evaluationscore_male_cf    <- getRecall(cf_df_male)
      recgap_genre_cf <- abs(
        mean_evaluationscore_female_cf - mean_evaluationscore_male_cf
      ) 
      
      message("Successfully Run Bootstap iteration")
      
      ret <- c( overall_evaluationsscore_cf, recgap_genre_cf ) 
      names(ret) <- c( "performance", "recgap" )
      return(ret)
    },
    error = function(e) {
      overall_evaluationsscore_cf <- NA
      recgap_genre_cf <- NA
      message("Got an error.  Returning NA, NA!")
      
      ret <- c( overall_evaluationsscore_cf, recgap_genre_cf ) 
      names(ret) <- c( "performance", "recgap" )
      return(ret)
    }, finally = {
      message("In finally block")
    }
  )
}

genre_books_reviews %>% 
  select(user_id, book_id, rating, gender, like_or_dislike) -> sampled_reviews


get_cf_boot_results <- function( cf_algo ) {
  
   boot_results_cf <- boot(
    data = sampled_reviews, 
    statistic = ReturnRecgap, 
    R = NUM_BOOT_REPLICATES, 
    the_algo = cf_algo
  )
  
  as.data.frame(boot_results_cf$t) %>% 
    rename(performance = V1, recgap = V2) %>% 
    filter( !is.na(performance) & !is.na(recgap) ) -> results_cf
  
  results_cf$algo <- cf_algo
  
  filename <- sprintf(
    "output/%s_cf_boot_results_%s_%d.csv", 
    BOOK_GENRE, 
    cf_algo, 
    NUM_BOOT_REPLICATES
  )
  message("Writing boot results file: ", filename)
  write.csv(results_cf, filename)
  
  return ( results_cf )
}

```



## Item-Based Collaborative Filtering (IBCF)

Here, we used Item-Based CF in order to make recommendations.  IBCF assumes 
that users will prefer items similar to other items that they like.  The
recommenderlab package figures out that two items are similar by calculating 
the statistical distance between them.  Here, we are using the cosine distance
between them to find out this distance.

```{r warning=FALSE, message=FALSE}
ibcf_boot_results <- get_cf_boot_results("IBCF")

head(ibcf_boot_results)
```

## User-Based Collaborative Filtering (UBCF)

UBCF tries to mimic word-of-mouth recommendations.  If a user U1 likes Books B1
and B2, chances are that user U2 will also like these books, if U1 and U2 are
similar to each other.  To find out whether two users are similar, the
recommenderlab package users k-nearest-neighbours to find similar users, using
cosine or pearson similarity.

```{r warning=FALSE, message=FALSE}
ubcf_boot_results <- get_cf_boot_results("UBCF")

head(ubcf_boot_results)
```

## SVD

This is just recommends everyone the same N books that are "most popular".  It
can be thought of as being similar to the Billboard Top 100

```{r warning=FALSE, message=FALSE}
svd_boot_results <- get_cf_boot_results("SVD")

head(svd_boot_results)
```

## RANDOM

This is just a random set of recommendations.  Just serves as a baseline

```{r warning=FALSE, message=FALSE}
random_boot_results <- get_cf_boot_results("RANDOM")

head(random_boot_results)
```

## POP

This is just recommends everyone the same N books that are "most popular".  It
can be thought of as being similar to the Billboard Top 100 etc.

```{r warning=FALSE, message=FALSE}
pop_boot_results <- get_cf_boot_results("POP")

head(pop_boot_results)
```


## Compare the algorithms!

We can now compare the algorithms with each other, visually.  We have up to
`NUM_BOOT_REPLICATES` results for each algo.  We can compare the means, and 
get confidence intervals, standard deviations and standard errors.

Credit:  https://www.r-graph-gallery.com/4-barplot-with-error-bar.html

```{r}
boot_results <- rbind(
  ibcf_boot_results, 
  ubcf_boot_results, 
  svd_boot_results, 
  pop_boot_results, 
  random_boot_results
)

sum_recgaps <- boot_results %>%
  group_by(algo) %>%
  summarise( 
    n=n(),
    mean_recgap=mean(recgap),
    sd_recgap=sd(recgap),
    mean_perf=mean(performance),
    sd_perf=sd(performance)
  ) %>%
  mutate( se=sd_recgap/sqrt(n))  %>%
  mutate( ic=se * qt((1-0.05)/2 + .5, n-1))
 
# Standard deviation
ggplot(sum_recgaps) +
  geom_bar( 
    aes(x=algo, y=mean_recgap), 
    stat="identity", 
    fill="forestgreen", 
    alpha=0.5) +
  geom_errorbar( 
    aes(x=algo, ymin=mean_recgap-sd_recgap, ymax=mean_recgap+sd_recgap), 
    width=0.4, 
    colour="orange", 
    alpha=0.9, 
    size=1.5) +
  ggtitle("using standard deviation")
 
# Standard Error
ggplot(sum_recgaps) +
  geom_bar( 
    aes(x=algo, y=mean_recgap), 
    stat="identity", 
    fill="forestgreen", 
    alpha=0.5) +
  geom_errorbar( 
    aes(x=algo, ymin=mean_recgap-se, ymax=mean_recgap+se), 
    width=0.4, 
    colour="orange", 
    alpha=0.9, 
    size=1.5) +
  ggtitle("using standard error")
 
# Confidence Interval
ggplot(sum_recgaps) +
  geom_bar( 
    aes(x=algo, y=mean_recgap), 
    stat="identity", 
    fill="forestgreen", 
    alpha=0.5) +
  geom_errorbar( 
    aes(x=algo, ymin=mean_recgap-ic, ymax=mean_recgap+ic), 
    width=0.4, 
    colour="orange", 
    alpha=0.9, 
    size=1.5) +
  ggtitle("using confidence interval")

```


## Recgap vs Performance

```{r}
ggplot(sum_recgaps, aes(x=mean_recgap, y=mean_perf, color=algo)) + 
    geom_point(size=6)
```


```{r}
cor(sum_recgaps$mean_perf, sum_recgaps$mean_recgap)
```

But is this correlation a pure coincidence?


```{r}
#TODO: Does this even make sense?
corPerm <- numeric(length = N_PERMUTATIONS)
for(i in 1:N_PERMUTATIONS)
{
 shufdata <- sum_recgaps[sample(nrow(sum_recgaps)),]
 corPerm[i] <- cor(shufdata$mean_perf, sum_recgaps$mean_recgap)
}
corObserved <- cor(sum_recgaps$mean_perf, sum_recgaps$mean_recgap)


p_value_Cor <- (sum(corPerm>=corObserved)+1)/length(corPerm)
p_value_Cor

hist(corPerm, xlim=range(c(corPerm,corObserved)))
abline(v=corObserved, col="red")
```

From the p-value and the histogram, we can see that the relationship between 
the recgap and the performance is **not** significant and could have totally 
occurred by chance!
